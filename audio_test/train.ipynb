{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ea47bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device :cpu\n",
      "there are 8732 samples in the dataset\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urbansounddataset import UrbanSoundDataset\n",
    "from cnn import CNNNetwork\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "prefix = \"C:/Users/ekrem/Desktop/makamFinder/pytorch_test/audio_test/Data/UrbanSound8K/\"\n",
    "ANNOTATIONS_FILE = prefix + \"metadata/UrbanSound8K.csv\"\n",
    "AUDIO_DIR = prefix + \"audio\"\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050 ## it means one seconds of audio\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model,data_loader,loss_fn,optimizer, device):\n",
    "    for inputs,targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        #calculate loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions,targets)\n",
    "        \n",
    "        #backpropagate loss and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    \n",
    "def train(model,data_loader,loss_fn,optimizer, device,epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_one_epoch(model,data_loader,loss_fn,optimizer, device)\n",
    "        print(\"------------------\")\n",
    "    print(\"Training is done.train_data_loader\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c15a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu device\n"
     ]
    }
   ],
   "source": [
    "##initiate data set\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"using {device} device\")\n",
    "\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "\n",
    "usd = UrbanSoundDataset (ANNOTATIONS_FILE, AUDIO_DIR, mel_spectogram,\n",
    "                        SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "\n",
    "train_data_loader = DataLoader(usd,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c153e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(cnn\u001b[38;5;241m.\u001b[39mparameters(),lr \u001b[38;5;241m=\u001b[39m LEARNING_RATE)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m##Train Model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(cnn\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel trained and saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, loss_fn, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining is done.train_data_loader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, data_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_one_epoch\u001b[39m(model,data_loader,loss_fn,optimizer, device):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs,targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     21\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m#calculate loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Desktop\\makamFinder\\pytorch_test\\audio_test\\urbansounddataset.py:34\u001b[0m, in \u001b[0;36mUrbanSoundDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     32\u001b[0m signal \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     33\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resample_if_necessary(signal,sr)\n\u001b[1;32m---> 34\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mix_down_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_right_pad_if_necessary(signal) \u001b[38;5;66;03m## if the number of samples is less then we want\u001b[39;00m\n\u001b[0;32m     36\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cut_if_necessary(signal) \u001b[38;5;66;03m## if the number of samples is more then we want\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\makamFinder\\pytorch_test\\audio_test\\urbansounddataset.py:60\u001b[0m, in \u001b[0;36mUrbanSoundDataset._mix_down_if_necessary\u001b[1;34m(self, signal)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mix_down_if_necessary\u001b[39m(\u001b[38;5;28mself\u001b[39m, signal):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m signal\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 60\u001b[0m         signal \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signal\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#build model\n",
    "cnn = CNNNetwork().to(device)\n",
    "## Lost function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(),lr = LEARNING_RATE)\n",
    "\n",
    "##Train Model\n",
    "train(cnn,train_data_loader, loss_fn, optimizer, device, EPOCHS)\n",
    "\n",
    "torch.save(cnn.state_dict(),\"cnn.pth\")\n",
    "print(\"model trained and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819b8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20178c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
